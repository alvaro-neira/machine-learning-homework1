# -*- coding: utf-8 -*-
"""5.2-cross-validation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KpWoc7Hntl7ukSml6oGHD_VL4Aw33OVU

# Validación Cruzada

Para poder evaluar el rendimiento de un conjunto de datos disminuyendo el sesgo de selección del conjunto de datos de entrenamiento, podemos realizar una serie de particiones del conjunto de datos para así obtener un rendimiento promedio de nuestro método al utilizar distintos conjuntos de entrenamiento y prueba. Esto también nos permite aplicar pruebas estadísticas a nuestras métricas y así poder seleccionar el mejor modelo con mayor certeza.
"""

import pandas as pd
import numpy as np
import sklearn.preprocessing
import sklearn.model_selection # Validación cruzada
import sklearn.linear_model
import sklearn.tree
import sklearn.neural_network # Perceptrón multicapa
import sklearn.svm
import sklearn.dummy
import scipy.stats # Prueba estadística
import matplotlib.pyplot as plt

np.random.seed(11) # Establecemos la semilla para el generador de números aleatorios

"""Importamos un conjunto de datos que contiene datos extraidos de una encuesta en donde se busca inferir el género de la persona en función de un grupo de características."""

gender = pd.read_csv("gender.csv")

"""Verificamos que las características son del tipo discretas."""

gender.head()

"""Exploramos cómo se distribuyen las características."""

gender.describe()

"""## Preprocesamiento"""

features = gender.iloc[:,:-1] # Seleccionamos sólo las características del conjunto de datos
label = gender["Gender"] # Seleccionamos la etiqueta del conjunto de datos
print(label)
"""Para poder entrenar nuestros modelos debemos transformar nuestras características a una representación vectorial. Para esto utilizamos el método One-Hot Encoding, el cual transforma cada característica en un vector de largo del número de categorías, este vector tiene un 1 en la posición de la categoría asociada a ese ejemplo."""

enc = sklearn.preprocessing.OneHotEncoder() # Instanciamos nuestro One-Hot Encoder
features_transformed = pd.DataFrame(
    enc.fit_transform(features).todense(), # Entrenamos nuestro transformador y transformamos las características
    columns = enc.get_feature_names()
)
features_transformed

features.iloc[0,:]

"""Pasamos de un espacio de 4 características a uno de 20 características."""

features_transformed.iloc[0,:]

"""## Modelamiento"""

def cross_val_score(**kwargs):
    """
    Recibe los argumentos para pasárselos a la función sklearn.model_selection.cross_validate
    Retorna una lista con los valores de AUCROC de cada una de las divisiones.
    """
    cv = sklearn.model_selection.cross_validate( # Esta función entrena un modelo para distintos subconjuntos generados al azar.
        scoring = 'roc_auc', # Usamos la medida de AUCROC para medir el rendimiento de los modelos
        cv = sklearn.model_selection.StratifiedKFold( # La división se realiza de manera estratificada
            n_splits = 10, # Creamos 10 subconjuntos
            shuffle = True, # Desordenamos los datos antes de dividirlos
        ),
        n_jobs = None, # Usamos sólo 1 worker para el entrenmiento
        **kwargs # Pasamos los argumentos recibidos por la función
    )
    return cv["test_score"]

"""Guardamos los valores de rendimiento para un conjunto de múltiples algoritmos de clasificación."""

lr_scores = cross_val_score(
    estimator = sklearn.linear_model.LogisticRegression(),
    X = features_transformed,
    y = label
)
lr_scores

tree_scores = cross_val_score(
    estimator = sklearn.tree.DecisionTreeClassifier(),
    X = features_transformed,
    y = label
)
tree_scores

mlp_scores = cross_val_score(
    estimator = sklearn.neural_network.MLPClassifier(max_iter=1000),
    X = features_transformed,
    y = label
)
mlp_scores

svm_scores = cross_val_score(
    estimator = sklearn.svm.SVC(),
    X = features_transformed,
    y = label
)
label

dummy_scores = cross_val_score(
    estimator = sklearn.dummy.DummyClassifier(strategy="stratified"),
    X = features_transformed,
    y = label
)
dummy_scores

"""## Conclusión"""

scores = [ # Guardamos todos los resultados en una lista
    lr_scores,
    tree_scores,
    mlp_scores,
    svm_scores,
    dummy_scores
]
names = [
    "Logistic Regression",
    "Decision Tree",
    "Multilayer Perceptron",
    "Support Vector Machine",
    "Dummy Classifier"
]

"""Con esta visualización podemos ver la distribución de resultados para cada uno de los algoritmos seleccionados."""

plt.boxplot(
    scores,
    labels = names
)
plt.xticks(rotation=45)
plt.show()

"""Extraemos el resultado promedio de cada uno de los algoritmos. Al parecer, según una estimación puntual, SVM es el algoritmo que mejor se comporta."""

dict(
    zip(
        names,
        map(
            np.mean,
            scores
        )
    )
)

"""Para verificar si la diferencia entre las medias de los resultados es estadísticamente significativa utilizamos una prueba de ANOVA.
No existen diferencias estadísticamente significativas entre cada uno de los algoritmos.
"""

scipy.stats.f_oneway(*scores)



